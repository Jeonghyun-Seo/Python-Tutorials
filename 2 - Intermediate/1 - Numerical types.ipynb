{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical types and error\n",
    "Click [here](https://datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fberkeley-physics%2FPython-Tutorials&urlpath=tree%2FPython-Tutorials%2F2+-+Intermediate%2F1+-+Numerical+types.ipynb&branch=master) to open this notebook in the DataHub.\n",
    "\n",
    "## Learning objectives\n",
    "By the end of this tutorial, you will be able to:\n",
    "- Distinguish and convert between numerical data types\n",
    "- Understand the concepts of overflow, underflow and roundoff error\n",
    "- Compare floating-point numbers and check for equality\n",
    "- Recognise and avoid sources of numerical error\n",
    "- Recognise and avoid errors caused by type mismatch\n",
    "\n",
    "## Relevant documentation\n",
    "- [Python data types](https://docs.python.org/3/library/stdtypes.html)\n",
    "- [NumPy data types](https://numpy.org/doc/stable/user/basics.types.html)\n",
    "- [Python float limitations tutorial](https://docs.python.org/3/tutorial/floatingpoint.html)\n",
    "\n",
    "## Numerical types\n",
    "You have seen native Python types, e.g. when inputting numbers directly into Python (these are known as literals). Determine the type of each of the variables defined below. (You might find the function `type(...)` useful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 3\n",
    "y = 9.5\n",
    "z = -4+3j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very often, we'll be using `NumPy`, which is mostly a Python wrapper for C++ code. This means that while the code runs much faster, the numbers are not Python types, but C++ types, which behave differently. Find the types of the variables `a`, `b`, `c`, and `d`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "zeros_array = np.zeros(5)\n",
    "linear_array = np.linspace(-1,1,10)\n",
    "example_list = [1,2,3,4]\n",
    "example_array = np.array(example_list)\n",
    "\n",
    "a = zeros_array[0]\n",
    "b = linear_array[0]\n",
    "c = example_list[0]\n",
    "d = example_array[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are these types different from those of native Python? Why?\n",
    "\n",
    "## Integers\n",
    "Integers are typically stored with no loss. If we choose to store an integer using $n$ bits, we can map these $n$ bits to $2^n$ integer values, typically from $-2^{n-1}$ to $2^{n-1}-1$, inclusive. For example, in a byte (8 bits), we can store one of 256 integers, which are conventionally chosen to be the range from -128 to 127.\n",
    "\n",
    "Numpy allows you to choose between 8-bit, 16-bit, 32-bit, 64-bit, and 128-bit integers, resorting to 64-bit integers by default on most systems. You can choose the data type when constructing an array, or explicitly convert numbers and arrays to another data type if required.\n",
    "\n",
    "Run the following cells and explain what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([-120,30,200,128], dtype=np.int8)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a+100, a-100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 64-bit integers (the usual default), the minimum and maximum integers are $-2^{63}=-9223372036854775808$ and $2^{63}-1=9223372036854775807$. (It is useful to remember than $2^{10}=1024\\approx1000= 10^3$, so $2^{63}\\approx 10^{19}$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([2])\n",
    "b.dtype #to check type of array (rather than type of element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b**62, b**63"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that C++-style integer arithmetic is equivalent to modular arithmetic, i.e. values differing by $2^n$ are indistinguishable. \n",
    "\n",
    "Python, on the other hand, automatically allots more bits to large integers, ensuring that integers remain exact and not modular. In line with the Python philosophy, this provides convenience at the cost of transparency and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**64, b**64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, when using NumPy integers, we must be careful not to have integers that are too large, since these may wrap around __without throwing an error__, which means you may not notice when your program goes wrong.\n",
    "\n",
    "## Floating-point numbers\n",
    "It may be intuitive to you that a digital computer with a finite amount of space cannot store an element of a continuous set. In 8 bits, we can store only 256 distinct values, which is far smaller than the \"number\" (the \"size\" of the set of) of real numbers. One data type that is meant to encode real numbers is called the _floating-point number,_ or _float_. The strategy here is to allot one bit for sign, $m$ bits for a binary fraction, and the remaining $n-m-1$ bits for an exponent. This means that the $2^{n}$ possible values map to, $$(-1)^s \\left(1+\\frac{f}{2^m}\\right)2^e,$$ where $s$ represents the sign bit, $f$ represents the binary fraction (an integer between $0$ and $2^m-1$), and $e$ represents the exponent (typically between $-2^{n-m-2}$ and $2^{n-m-2}-1$). (In practice, several \"special\" values like `inf` and `nan` are reserved.)\n",
    "\n",
    "For the 64-bit float, which is the typical default for both Python and NumPy, $m=52$. This means that 11 bits are reserved for the exponent, i.e. it has values from -1024 to 1023. Thus the smallest float (in magnitude) is $2^{-1074}\\approx 10^{-325}$ (since for small numbers the leading 1 of the binary fraction can be ignored), and the largest float is slightly below $2^{1024}\\approx 10^{308}$. When we attempt to do floating-point arithmetic whose result is outside of the range of numbers representable by the floats we are using, that is known as _overflow_ or _underflow_ (for numbers that are too large and small respectively). Run the following cells and explain the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2.0\n",
    "y = np.array(x, dtype=np.float64)\n",
    "z = y.astype(np.float128) #copying y to z, but changing type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x**-1074, x**-1075 #try with y and z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x**1023*1.9999)\n",
    "print(x**1024) \n",
    "#try with y and z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using a finite list of values to represent an infinite set of numbers, each float value corresponds to a range of numbers. Specifically, it specifies values within a relative range of $2^{-m}$. For example, the closest float representing the numbers $1\\pm 2^{-53}$ is 1. This is known as _roundoff error_. It implies that all floats are accurate only to a factor of $2^{-52}\\approx 10^{-16}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2**-53\n",
    "y = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x + y == y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that when we add numbers with different magnitudes, we lose accuracy. This also holds for small differences between large numbers (for example, if we want to estimate the derivative of a function by comparing its values at neighbouring points, we might lose accuracy by taking points that are too close together)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1.23456789\n",
    "y = 9.87654321\n",
    "print(y-x)\n",
    "\n",
    "z = 10**10\n",
    "print((z+y)-(z+x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also means that testing floats for equality can be tricky; consider the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7 * 0.4 == 0.28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual strategy to do this is to check if the two floats agree to some (arbitrary) precision. Why is it better to use a relative tolerance than an absolute tolerance? (E.g. compare the difference between 1000000000000000.0 and 1000000000000000.1 to that between 1.0 and 1.001) Implement the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_equal(x, y, tol=1e-6):\n",
    "    \"\"\"Returns True if x/y is within the range [1-tol,1+tol], False otherwise.\"\"\"\n",
    "    #implement me\n",
    "    return False\n",
    "    \n",
    "float_equal(0.7*0.4, 0.28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that all floats are exact binary fractions, and wonder why, since all integers (not just binary ones) can be represented exactly, all rational numbers are not represented exactly. You can further pursue this idea in the tutorial on abstract objects.\n",
    "\n",
    "Now try calculating $n!$ for various $n$. If we want exact answers, should we use floats or ints? Try both and judge the error (if any). If you use a NumPy int, what is the highest $n$ whose factorial you can calculate exactly? How about a Python int? What is the highest $n$ whose factorial you can compute as a float, before running into overflow? Calculate $\\log n!$ for even higher $n$. How would you use this in further calculations, e.g. in finding the number of combinations of $m$ objects from $n$ choices? Which data type computes faster?\n",
    "\n",
    "In general, going to log-space is a good idea if you are working with extremely large numbers (e.g. calculate the entropy instead of the number of microstates).\n",
    "\n",
    "## Changing types\n",
    "Python (and NumPy) automatically (implicitly) converts types where necessary to preserve information; see the examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(2+0.5)\n",
    "print(4*2)\n",
    "print(4/2)\n",
    "print(np.array([4,4])/np.array([1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you use this feature, e.g. to convert the integer 2 to a float?\n",
    "\n",
    "You can also explicitly change types using the `int(...)` and `float(...)` functions, or the NumPy `asType` method as seen above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(3.5), int(round(3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boolean type (True/False values) converts to 1/0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(False), float(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool(12.3), bool(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the following function using as little code as possible (_code golf_):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bool_convert(b):\n",
    "    \"\"\"Returns the integer 27 if b is True, and 0 if b is False\"\"\"\n",
    "\n",
    "bool_convert(True), bool_convert(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is not typed, which means variables can be reassigned to any type, and parameter types don't have to be specifield. In the `bool_convert(b)` function above, we have assumed that `b` is a Boolean, but this is not enforced: we may pass a float or an int, and the function will still attempt to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_convert(10), bool_convert(0.67)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this weak type enforcement provides greater convenience by allowing us to write functions that accept several input types, it can lead to errors when we use an unexpected type without realising, since some input types will cause the program to crash or have unexpected behaviour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_convert(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gracefully handle unexpected input types, you can explicitly check the type of each relevant parameter. The `assert` statement (find the relevant docs!) throws an error when a condition isn't met. Try enforcing the _precondition_ that the argument to the `bool_convert` function must be a Boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 3\n",
    "assert type(x) == float, \"x is not a float!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
